# vLLM v0.10.1.1 Default Configuration

# Model serving settings
model_name: "meta-llama/Meta-Llama-3.1-8B-Instruct"
served_model_name: "llama-3.1-8b"
max_model_len: 4096
tensor_parallel_size: 1

# V1 Engine Configuration
use_v1_engine: true
attention_backend: "FLASHINFER"
enable_prefix_caching: true

# GPU settings
gpu_memory_utilization: 0.9
trust_remote_code: true

# Performance settings
max_num_batched_tokens: 8192
max_num_seqs: 256

# API settings
host: "0.0.0.0"
port: 8000
disable_log_stats: false
disable_log_requests: false

# Advanced RTX 4090 optimizations
enable_chunked_prefill: true
max_num_batched_tokens: 8192
max_paddings: 512

# Quantization (uncomment if using quantized models)
# quantization: "awq"  # or "gptq", "fp8", etc.

# Multi-modal (uncomment if using vision models)
# image_input_type: "pixel_values"
# image_token_id: 128256
# image_input_shape: "1,3,336,336"
