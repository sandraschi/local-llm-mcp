# vLLM v0.10.1.1 Docker Environment Configuration
# Copy this file to .env and customize for your setup

# =============================================================================
# GPU CONFIGURATION
# =============================================================================
CUDA_VISIBLE_DEVICES=0
VLLM_GPU_MEMORY_UTILIZATION=0.9
TORCH_CUDA_ARCH_LIST=8.9

# =============================================================================
# vLLM V1 ENGINE CONFIGURATION
# =============================================================================
VLLM_USE_V1=1
VLLM_ATTENTION_BACKEND=FLASHINFER
VLLM_ENABLE_PREFIX_CACHING=1
VLLM_FLASH_ATTN_VERSION=2

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================
VLLM_MAX_NUM_BATCHED_TOKENS=8192
VLLM_MAX_NUM_SEQS=256
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
VLLM_SERVED_MODEL_NAME=llama-3.1-8b
VLLM_MAX_MODEL_LEN=4096
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_TRUST_REMOTE_CODE=true

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================
VLLM_PORT=8000
MCP_PORT=3001
VLLM_HOST=0.0.0.0
MCP_HOST=0.0.0.0
