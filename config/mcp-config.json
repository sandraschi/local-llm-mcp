{
  "server": {
    "name": "local-llm-mcp",
    "version": "1.0.0",
    "description": "Local LLM MCP Server with vLLM v0.10.1.1 support",
    "host": "0.0.0.0",
    "port": 3001,
    "log_level": "INFO"
  },
  "providers": {
    "vllm_v10": {
      "enabled": true,
      "base_url": "http://vllm-v10:8000",
      "api_type": "openai",
      "models": [
        {
          "name": "llama-3.1-8b",
          "display_name": "Llama 3.1 8B Instruct",
          "context_length": 4096,
          "max_tokens": 2048
        }
      ],
      "health_check": {
        "enabled": true,
        "endpoint": "/health",
        "interval": 30,
        "timeout": 10
      }
    }
  }
}
